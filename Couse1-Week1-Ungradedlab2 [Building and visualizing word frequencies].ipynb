{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Visualizing word frequencies\n",
    "\n",
    "In this lab, we will focus on the build_freqs() helper function and visualizing a dataset fed into it. In our goal of tweet sentiment analysis, this function will build a dictionary where we can lookup how many times a word appears in the lists of positive or negative tweets. This will be very helpful when extracting the features of the dataset in the week's programming assignment. Let's see how this function is implemented under the hood in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of tweets :  10000\n"
    }
   ],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "print(\"number of tweets : \",len(tweets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will build a label array that matches our tweets.\n",
    "This array will be composed of 10000 elements\n",
    "The first 5000 will be filled with 1 labels denoting positiv sentments, while the next 5000 will be 0 labels denoting negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.ones(len(all_positive_tweets)),np.zeros(len(all_negative_tweets)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "labels[9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'key1' : 1 , 'key2': 2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'key1': 0, 'key2': 2, 'key3': -4}\n"
    }
   ],
   "source": [
    "#adding new entry\n",
    "dictionary['key3'] = -4\n",
    "\n",
    "#overwritng thevalue of key1\n",
    "dictionary['key1'] = 0\n",
    "\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessing values and lookup keys\n",
    "- we can use the square breacket notation \n",
    "- get() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2\n"
    }
   ],
   "source": [
    "print(dictionary['key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'key9'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-d6d330999aba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'key9'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'key9'"
     ]
    }
   ],
   "source": [
    "print(dictionary['key9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "item found :  0\nitem found :  0\n"
    }
   ],
   "source": [
    "#this prints a value\n",
    "if 'key1' in dictionary:\n",
    "    print(\"item found : \",dictionary['key1'])\n",
    "\n",
    "else:\n",
    "    print('key1 is not defined')\n",
    "\n",
    "#same but with the get function\n",
    "print(\"item found : \" , dictionary.get('key1' , -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "key does not exist!\n-1\n"
    }
   ],
   "source": [
    "# This prints a message because the key is not found\n",
    "if 'key7' in dictionary:\n",
    "    print(dictionary['key7'])\n",
    "else:\n",
    "    print('key does not exist!')\n",
    "\n",
    "# This prints -1 because the key is not found and we set the default to -1\n",
    "print(dictionary.get('key7', -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LETS TAKE A LOOK AT BUILD_FREQS() FUNCTION THAT IS INBUILT IN COURSERA NOTEBOOK.\n",
    "THIS FUNCTION CREATES THE DICTIONARY CONTAINING THE WORD COUNTS FROM EACH CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', 'â€¦']\n"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def process_tweets(tweet):\n",
    "    #remove the old style retweet text \"RT\"\n",
    "    tweet2 = re.sub( r'^RT[\\s]+' , '' ,tweet)\n",
    "\n",
    "    #remove hyperlinks\n",
    "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*' , '' ,      tweet2)\n",
    "\n",
    "    #remove hashtag (only the # sign)\n",
    "    tweet2 = re.sub(r'#' , '' ,tweet2)\n",
    "\n",
    "    #instantiate the tokenizer class\n",
    "    tokenizer = TweetTokenizer  (preserve_case=False,\n",
    "                          strip_handles=True,\n",
    "                          reduce_len = True)\n",
    "\n",
    "    #tokenize tweets\n",
    "    tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "\n",
    "    #importing the english stop words from nltk\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    tweets_clean = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if(word not in stopwords_english \n",
    "        and \n",
    "        word not in string.punctuation):\n",
    "            tweets_clean.append(word)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    #create an empty list to store the stems\n",
    "    tweets_stem = []\n",
    "\n",
    "    for word in tweets_clean:\n",
    "        stem_word = stemmer.stem(word)  \n",
    "        #stemming word\n",
    "        tweets_stem.append(stem_word)\n",
    "    \n",
    "    return tweets_stem \n",
    "\n",
    "print(process_tweets(tweets[2277]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets , ys):\n",
    "    \"\"\"Build frequencies\n",
    "    input:\n",
    "        tweets : a list of tweets\n",
    "        ys: an mx1 array with the sentiment label of each tweet(either 0 or 1)\n",
    "    output:\n",
    "        freqs: a dictionary mapping each (word,sentiment) pair to its frequency\n",
    "        \"\"\"\n",
    "    \n",
    "    #convert the np array to list since zip needs an iterble\n",
    "    # The squeeze is necessary or the list ends up with  one element\n",
    "    # also note that this is just a NOP if ys is already a list\n",
    "     \n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    #start with an empty dictionary and populate it by looping  over all tweets\n",
    "    # and over all processed words in each tweet\n",
    "    freqs={}\n",
    "\n",
    "    for y , tweet in zip(yslist , tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair =  (word,y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LETS UNDERSTAND THE LOGIC BEHIND THIS \n",
    "\n",
    "### \"folowfriday\" appears 25 times in the positive tweets\n",
    "('followfriday', 1.0): 25\n",
    "\n",
    "### \"shame\" appears 19 times in the negative tweets\n",
    "('shame', 0.0): 19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3NLP",
   "language": "python",
   "name": "python3nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}